{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f9788cc-f741-4605-bab2-c6f897599a86",
   "metadata": {},
   "source": [
    "# Minio - Deploy Models (BERT and NeuralLog) to Kserve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdf4a54-5808-4ab2-82a6-e42895ee6455",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pydantic\n",
    "!pip3 install \"ray[serve]==1.10.0\"\n",
    "!pip install kserve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0cab83cc-3697-4207-b36e-2e4be9f4d3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from kubernetes import client\n",
    "from kserve import KServeClient\n",
    "from kserve import constants\n",
    "from kserve import V1beta1PredictorSpec\n",
    "from kserve import V1beta1TFServingSpec\n",
    "from kserve import V1beta1InferenceServiceSpec\n",
    "from kserve import V1beta1InferenceService"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bacd25a0-b501-46a8-8c3a-a29fb0aa8741",
   "metadata": {},
   "outputs": [],
   "source": [
    "kserve = KServeClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "82336666-a52e-4fb7-8bd4-50b1a73fc2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = os.getenv(\"BUCKET_NAME\",\"ml-data\")\n",
    "S3_SERVICE_ACCOUNT = os.getenv(\"S3_SERVICE_ACCOUNT\", \"minio-sa\")\n",
    "NAMESPACE = os.getenv(\"NAMESPACE\", 'kubeflow-user-example-com')\n",
    "BERT_INFERENCE_NAME = os.getenv(\"BERT_INFERENCE_NAME\", 'bert-minio')\n",
    "LOG_ANOMOLY_INFERENCE_NAME = os.getenv(\"LOG_ANOMOLY_INFERENCE_NAME\", 'log-bert-minio')\n",
    "BERT_MODEL_URL = os.getenv(\"S3_BERT_MODEL_URL\", f\"s3://{bucket}logs/saved_models/tfs/bert_model/\")\n",
    "LOG_ANOMOLY_MODEL_URL = os.getenv(\"LOG_ANOMOLY_MODEL_URL\", f\"s3://{bucket}/logs/saved_models/tfs/log_model\")\n",
    "TF_RUNTIME_VERSION = os.getenv(\"TF_RUNTIME_VERSION\", \"2.8.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c687c0-6596-4a6e-b8e8-8b6919bf6709",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d11af43e-3194-4db7-92f8-f4ae61122214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK IF SERVICE EXISTS\n",
    "def service_exists(namespace, inference_name):\n",
    "    inference_list = kserve.get(namespace=namespace)\n",
    "    for item in inference_list.items():\n",
    "        if item[0] == 'items':\n",
    "            for item_inference in item[1]:\n",
    "                if item_inference['metadata']['name'] == inference_name:\n",
    "                    return True\n",
    "    return False\n",
    "\n",
    "\n",
    "# CREATE INFERENCE SERVICE\n",
    "def create_inference_service(service_account, storage_url, runtime_version, namespace, name):\n",
    "    # MODEL SPECIFICATION\n",
    "    default_model_spec = V1beta1InferenceServiceSpec(\n",
    "        predictor=V1beta1PredictorSpec(\n",
    "            service_account_name=service_account,\n",
    "            tensorflow=V1beta1TFServingSpec(\n",
    "                storage_uri=storage_url,\n",
    "                runtime_version=runtime_version\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # INFERENCE SERVICE SPECIFICATION\n",
    "    isvc = V1beta1InferenceService(api_version=constants.KSERVE_V1BETA1,\n",
    "                              kind=constants.KSERVE_KIND,\n",
    "                              metadata=client.V1ObjectMeta(name=name, \n",
    "                                                           namespace=namespace,\n",
    "                                                           labels={name: \"yes\"}),\n",
    "                              spec=default_model_spec)\n",
    "    kserve.create(isvc)\n",
    "    \n",
    "    \n",
    "# RESTART INFERENCE SERVICE PODS\n",
    "def restart_service(namespace, service_name):\n",
    "    v1 = client.CoreV1Api()\n",
    "    result = v1.list_namespaced_pod( namespace, label_selector=f\"{service_name}=yes\", watch=False)\n",
    "    for pod in result.items:\n",
    "        v1.delete_namespaced_pod(pod.metadata.name, pod.metadata.namespace)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff17adef-8946-4199-bdf5-1dc2155ee0cc",
   "metadata": {},
   "source": [
    "## Deploy BERT Embeddings model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1b8eff04-ec98-4047-a573-51b5df63290d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK IF THE SERVICE EXISTS IF NOT CREATE IT\n",
    "bert_inference_exists = service_exists(NAMESPACE, BERT_INFERENCE_NAME)\n",
    "if bert_inference_exists:\n",
    "    restart_service(NAMESPACE, BERT_INFERENCE_NAME)\n",
    "else:\n",
    "    create_inference_service(S3_SERVICE_ACCOUNT, BERT_MODEL_URL, TF_RUNTIME_VERSION, NAMESPACE, BERT_INFERENCE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308068a9-2b6f-45f0-b69e-1f23d100cde1",
   "metadata": {},
   "source": [
    "## Deploy Log Anomoly model that was trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c2904b04-d13a-469a-be87-92809f17a7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK IF THE SERVICE EXISTS IF NOT CREATE IT\n",
    "log_inference_exists = service_exists(NAMESPACE, LOG_ANOMOLY_INFERENCE_NAME)\n",
    "if log_inference_exists:\n",
    "    restart_service(NAMESPACE, LOG_ANOMOLY_INFERENCE_NAME)\n",
    "else:\n",
    "    create_inference_service(S3_SERVICE_ACCOUNT, LOG_ANOMOLY_MODEL_URL, TF_RUNTIME_VERSION, NAMESPACE, LOG_ANOMOLY_INFERENCE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d796d68b-50cf-4a30-a860-63110b97015b",
   "metadata": {},
   "source": [
    "## Wait till service is ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "da904890-2f18-4d0c-bf17-4794b2dd6b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "kserve.wait_isvc_ready(BERT_INFERENCE_NAME, namespace=NAMESPACE)\n",
    "kserve.wait_isvc_ready(LOG_ANOMOLY_INFERENCE_NAME, namespace=NAMESPACE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
