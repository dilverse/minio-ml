{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0136c3d-d9dd-40f2-a257-1a585c312990",
   "metadata": {},
   "source": [
    "# Log Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e86015d-9458-4b1a-aefc-4ce3bd65e319",
   "metadata": {},
   "source": [
    "## Config definition initialization\n",
    "\n",
    "This config will be used later to pull logs and other related data from Minio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be58147-1c72-4cce-8466-a31cce984772",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49bf859-db21-42bc-aff4-f845aa6acb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fsspec\n",
    "import s3fs\n",
    "import pandas as pd\n",
    "# initialize minio config for fsspec to be used downstream\n",
    "fsspec.config.conf = {\n",
    "  \"s3\":\n",
    "  {\n",
    "    \"key\": \"console\",\n",
    "    \"secret\": \"console123\",\n",
    "    \"client_kwargs\": {\n",
    "      \"endpoint_url\": \"https://ns-1.m.dc.min.dev\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "s3 = s3fs.S3FileSystem()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d6a095-1ef7-45d7-b8eb-194632efae7c",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabcb871-40fc-4031-8484-f44cd5532229",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "#remote paths\n",
    "model_remote_path = \"s3://ml-data/logs/saved_models/bert-base-uncased/\"\n",
    "logs_remote_path = \"s3://ml-data/logs/data/raw/HDFS/HDFS.log\"\n",
    "logs_labelled_remote_path = \"s3://ml-data/logs/data/raw/HDFS/anomaly_label.csv\"\n",
    "#local paths\n",
    "model_path = \"model/\"\n",
    "logs_path = \"logs/data/hdfs.log\"\n",
    "logs_labelled_path = \"logs/data/anomaly_label.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df92091-7827-4261-9ea6-6c626b473e31",
   "metadata": {},
   "source": [
    "## Download data to local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f539fed6-40bf-4cd9-87d7-9a162a9dae99",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.download(rpath=model_remote_path, lpath=model_path, recursive=True)\n",
    "s3.download(rpath=logs_remote_path, lpath=logs_path)\n",
    "s3.download(rpath=logs_labelled_remote_path, lpath=logs_labelled_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecac90d-f246-41a2-882f-f0f3aded1e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9536dbbe-8548-4b16-a723-78fcc037a622",
   "metadata": {},
   "source": [
    "## Load Bert Model to generate embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ca7e57-e3fe-43ad-846a-76149f698894",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertModel\n",
    "import tensorflow as tf\n",
    "s3.download(rpath=model_remote_path, lpath=model_path, recursive=True)\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "bert_model = TFBertModel.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f5c599-1641-4380-b528-0191d6ae43fa",
   "metadata": {},
   "source": [
    "## Helper functions for Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b17ea8-392b-4eeb-912f-5eb81b9509d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(s):\n",
    "    \"\"\" Preprocess log message\n",
    "    Parameters\n",
    "    ----------\n",
    "    s: str, raw log message\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str, preprocessed log message without number tokens and special characters\n",
    "    \"\"\"\n",
    "    # s = re.sub(r'(\\d+\\.){3}\\d+(:\\d+)?', \" \", s)\n",
    "    # s = re.sub(r'(\\/.*?\\.[\\S:]+)', ' ', s)\n",
    "    s = re.sub('\\]|\\[|\\)|\\(|\\=|\\,|\\;', ' ', s)\n",
    "    s = \" \".join([word.lower() if word.isupper() else word for word in s.strip().split()])\n",
    "    s = re.sub('([A-Z][a-z]+)', r' \\1', re.sub('([A-Z]+)', r' \\1', s))\n",
    "    s = \" \".join([word for word in s.split() if not bool(re.search(r'\\d', word))])\n",
    "    trantab = str.maketrans(dict.fromkeys(list(string.punctuation)))\n",
    "    content = s.translate(trantab)\n",
    "    s = \" \".join([word.lower().strip() for word in content.strip().split()])\n",
    "    return s\n",
    "\n",
    "def bert_encoder(s, no_wordpiece=0):\n",
    "    \"\"\" Compute semantic vector with BERT\n",
    "    Parameters\n",
    "    ----------\n",
    "    s: string to encode\n",
    "    no_wordpiece: 1 if you do not use sub-word tokenization, otherwise 0\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        np array in shape of (768,)\n",
    "    \"\"\"\n",
    "    if no_wordpiece:\n",
    "        words = s.split(\" \")\n",
    "        words = [word for word in words if word in bert_tokenizer.vocab.keys()]\n",
    "        s = \" \".join(words)\n",
    "    inputs = bert_tokenizer(s, return_tensors='tf', max_length=512)\n",
    "    outputs = bert_model(**inputs)\n",
    "    v = tf.reduce_mean(outputs.last_hidden_state, 1)\n",
    "    return v[0]\n",
    "\n",
    "def _split_data(x_data, y_data=None, train_ratio=0, split_type='uniform'):\n",
    "    \"\"\" Split train/test data\n",
    "    Parameters\n",
    "    ----------\n",
    "    x_data: list, set of log sequences (in the type of semantic vectors)\n",
    "    y_data: list, labels for each log sequence\n",
    "    train_ratio: float, training ratio (e.g., 0.8)\n",
    "    split_type: `uniform` or `sequential`, which determines how to split dataset. `uniform` means\n",
    "            to split positive samples and negative samples equally when setting label_file. `sequential`\n",
    "            means to split the data sequentially without label_file. That is, the first part is for training,\n",
    "            while the second part is for testing.\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    \"\"\"\n",
    "    (x_data, y_data) = shuffle(x_data, y_data)\n",
    "    if split_type == 'uniform' and y_data is not None:\n",
    "        pos_idx = y_data > 0\n",
    "        x_pos = x_data[pos_idx]\n",
    "        y_pos = y_data[pos_idx]\n",
    "        x_neg = x_data[~pos_idx]\n",
    "        y_neg = y_data[~pos_idx]\n",
    "        train_pos = int(train_ratio * x_pos.shape[0])\n",
    "        train_neg = train_pos\n",
    "        x_train = np.hstack([x_pos[0:train_pos], x_neg[0:train_neg]])\n",
    "        y_train = np.hstack([y_pos[0:train_pos], y_neg[0:train_neg]])\n",
    "        x_test = np.hstack([x_pos[train_pos:], x_neg[train_neg:]])\n",
    "        y_test = np.hstack([y_pos[train_pos:], y_neg[train_neg:]])\n",
    "    elif split_type == 'sequential':\n",
    "        num_train = int(train_ratio * x_data.shape[0])\n",
    "        x_train = x_data[0:num_train]\n",
    "        x_test = x_data[num_train:]\n",
    "        if y_data is None:\n",
    "            y_train = None\n",
    "            y_test = None\n",
    "        else:\n",
    "            y_train = y_data[0:num_train]\n",
    "            y_test = y_data[num_train:]\n",
    "    # Random shuffle\n",
    "    indexes = shuffle(np.arange(x_train.shape[0]))\n",
    "    x_train = x_train[indexes]\n",
    "    if y_train is not None:\n",
    "        y_train = y_train[indexes]\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "\n",
    "def load_HDFS(log_file, label_file=None, train_ratio=0.5, window='session',\n",
    "              split_type='uniform', e_type=\"bert\", no_word_piece=0):\n",
    "    \"\"\" Load HDFS unstructured log into train and test data\n",
    "    Arguments\n",
    "    ---------\n",
    "        log_file: str, the file path of raw log (extension: .log).\n",
    "        label_file: str, the file path of anomaly labels (extension: .csv).\n",
    "        train_ratio: float, the ratio of training data for train/test split.\n",
    "        window: str, the window options including `session` (default).\n",
    "        split_type: `uniform` or `sequential`, which determines how to split dataset. `uniform` means\n",
    "            to split positive samples and negative samples equally when setting label_file. `sequential`\n",
    "            means to split the data sequentially without label_file. That is, the first part is for training,\n",
    "            while the second part is for testing.\n",
    "        e_type: str, embedding type (choose from BERT, XLM, and GPT2).\n",
    "        no_word_piece: bool, use split word into wordpiece or not.\n",
    "    Returns\n",
    "    -------\n",
    "        (x_train, y_train): the training data\n",
    "        (x_test, y_test): the testing data\n",
    "    \"\"\"\n",
    "\n",
    "    print('====== Input data summary ======')\n",
    "\n",
    "    e_type = e_type.lower()\n",
    "    if e_type == \"bert\":\n",
    "        encoder = bert_encoder\n",
    "    elif e_type == \"xlm\":\n",
    "        encoder = xlm_encoder\n",
    "    else:\n",
    "        if e_type == \"gpt2\":\n",
    "            encoder = gpt2_encoder\n",
    "        else:\n",
    "            raise ValueError('Embedding type {0} is not in BERT, XLM, and GPT2'.format(e_type.upper()))\n",
    "\n",
    "    E = {}\n",
    "    t0 = time.time()\n",
    "    assert log_file.endswith('.log'), \"Missing .log file\"\n",
    "    # elif log_file.endswith('.log'):\n",
    "    assert window == 'session', \"Only window=session is supported for HDFS dataset.\"\n",
    "    print(\"Loading\", log_file)\n",
    "    with open(log_file, mode=\"r\", encoding='utf8') as f:\n",
    "        logs = f.readlines()\n",
    "        logs = [x.strip() for x in logs]\n",
    "    data_dict = OrderedDict()\n",
    "    n_logs = len(logs)\n",
    "    print(n_logs)\n",
    "    print(\"Loaded\", n_logs, \"lines!\")\n",
    "    for line in tqdm(logs):\n",
    "        blkId_list = re.findall(r'(blk_-?\\d+)', line)\n",
    "        blkId_list = list(set(blkId_list))\n",
    "        if len(blkId_list) >= 2:\n",
    "            continue\n",
    "        blkId_set = set(blkId_list)\n",
    "        content = clean(line).lower()\n",
    "        if content not in E.keys():\n",
    "            E[content] = encoder(content, no_word_piece)\n",
    "        for blk_Id in blkId_set:\n",
    "            if not blk_Id in data_dict:\n",
    "                data_dict[blk_Id] = []\n",
    "            data_dict[blk_Id].append(E[content])\n",
    "    data_df = pd.DataFrame(list(data_dict.items()), columns=['BlockId', 'EventSequence'])\n",
    "\n",
    "    if label_file:\n",
    "        # Split training and validation set in a class-uniform way\n",
    "        label_data = pd.read_csv(label_file, engine='c', na_filter=False, memory_map=True)\n",
    "        label_data = label_data.set_index('BlockId')\n",
    "        label_dict = label_data['Label'].to_dict()\n",
    "        data_df['Label'] = data_df['BlockId'].apply(lambda x: 1 if label_dict[x] == 'Anomaly' else 0)\n",
    "        print(\"Saving data...\")\n",
    "        np.savez_compressed(\"data-{0}.npz\".format(e_type), data_x=data_df['EventSequence'].values,\n",
    "                            data_y=data_df['Label'].values)\n",
    "        # Split train and test data\n",
    "        (x_train, y_train), (x_test, y_test) = _split_data(data_df['EventSequence'].values,\n",
    "                                                           data_df['Label'].values, train_ratio, split_type)\n",
    "\n",
    "        print(y_train.sum(), y_test.sum())\n",
    "    # else:\n",
    "    #     raise NotImplementedError(\"Missing label file for the HDFS dataset!\")\n",
    "\n",
    "    if label_file is None:\n",
    "        if split_type == 'uniform':\n",
    "            split_type = 'sequential'\n",
    "            print('Warning: Only split_type=sequential is supported if label_file=None.'.format(split_type))\n",
    "        # Split training and validation set sequentially\n",
    "        x_data = data_df['EventSequence'].values\n",
    "        (x_train, _), (x_test, _) = _split_data(x_data, train_ratio=train_ratio, split_type=split_type)\n",
    "        print('Total: {} instances, train: {} instances, test: {} instances'.format(x_data.shape[0], x_train.shape[0], x_test.shape[0]))\n",
    "        return (x_train, None), (x_test, None), data_df\n",
    "    # else:\n",
    "    #     raise NotImplementedError('load_HDFS() only support csv and npz files!')\n",
    "    print(\"Loaded all HDFS dataset in: \", time.time() - t0)\n",
    "\n",
    "    num_train = x_train.shape[0]\n",
    "    num_test = x_test.shape[0]\n",
    "    num_total = num_train + num_test\n",
    "    num_train_pos = sum(y_train)\n",
    "    num_test_pos = sum(y_test)\n",
    "    num_pos = num_train_pos + num_test_pos\n",
    "\n",
    "    print('Total: {} instances, {} anomaly, {} normal'.format(num_total, num_pos, num_total - num_pos))\n",
    "    print('Train: {} instances, {} anomaly, {} normal'.format(num_train, num_train_pos, num_train - num_train_pos))\n",
    "    print('Test: {} instances, {} anomaly, {} normal'.format(num_test, num_test_pos, num_test - num_test_pos))\n",
    "\n",
    "    return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a929c2c-b5dc-4814-8452-d90b4ae04c69",
   "metadata": {},
   "source": [
    "## Preporcess data and generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84e6fd0-cf23-4966-8340-ede65ce2c6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_tr, y_tr), (x_te, y_te) = load_HDFS(log_file=logs_path, \n",
    "                                       label_file=logs_labelled_path,\n",
    "                                       split_type='sequential',\n",
    "                                       train_ratio=0.8,\n",
    "                                       e_type='bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b47047-36cf-4363-9ddf-132c343d56eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed(\"processed_train_data.npz\", x_train=x_tr, y_train=y_tr)\n",
    "np.savez_compressed(\"processed_eval_data.npz\", x_test=x_te, y_test=y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412c8237-4ffa-4c7d-9e94-4450226bb604",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
