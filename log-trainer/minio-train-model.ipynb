{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f5c37a2-a9c8-45a9-828a-14ddeacb630f",
   "metadata": {},
   "source": [
    "# Train Anamoly Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0a8f2d-22f4-46d9-888f-5dd6a6273dc0",
   "metadata": {},
   "source": [
    "## Initialize Parameters from Data-processing notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ea7100-a648-4e01-be7c-ffbe1084f175",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d919af-bd8d-4d4b-9bec-68c8a70a29f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from official.nlp import optimization\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea887882-d58f-4881-9423-a0d21ffe6c79",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "np_data = np.load('processed_train_data.npz', allow_pickle=True)\n",
    "x_tr = np_data['x_train']\n",
    "y_tr = np_data['y_train']\n",
    "embed_dim = 768\n",
    "max_len = 75\n",
    "fine_tune_model_remote_path = None\n",
    "batch_size = 64\n",
    "epochs = 1, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b0ba9b-f6cc-4e12-9539-4a74290c0240",
   "metadata": {},
   "source": [
    "## Helper Functions for Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dc6bea-ba32-47cc-91b9-2b3897fb92c3",
   "metadata": {},
   "source": [
    "### Create positional embedding layer (Custom Layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d037a5-4dfe-435f-8a04-49fb78b8d7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                            np.arange(d_model)[np.newaxis, :],\n",
    "                            d_model)\n",
    "\n",
    "    # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "class PositionEmbedding(layers.Layer):\n",
    "    def __init__(self, max_len, vocab_size, embed_dim):\n",
    "        super(PositionEmbedding, self).__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_encoding = positional_encoding(max_len,\n",
    "                                                embed_dim)\n",
    " \n",
    "    def call(self, x):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c4e222-b57d-4289-8d11-66dff66c4ed3",
   "metadata": {},
   "source": [
    "### Batch generator class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5b2360-c356-42d4-a6f6-6ba96687919a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchGenerator(Sequence):\n",
    "\n",
    "    def __init__(self, X, Y, batch_size):\n",
    "        self.X, self.Y = X, Y\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.X) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # print(self.batch_size)\n",
    "        dummy = np.zeros(shape=(embed_dim,))\n",
    "        x = self.X[idx * self.batch_size:min((idx + 1) * self.batch_size, len(self.X))]\n",
    "        X = np.zeros((len(x), max_len, embed_dim))\n",
    "        Y = np.zeros((len(x), 2))\n",
    "        item_count = 0\n",
    "        for i in range(idx * self.batch_size, min((idx + 1) * self.batch_size, len(self.X))):\n",
    "            x = self.X[i]\n",
    "            if len(x) > max_len:\n",
    "                x = x[-max_len:]\n",
    "            x = np.pad(np.array(x), pad_width=((max_len - len(x), 0), (0, 0)), mode='constant',\n",
    "                       constant_values=0)\n",
    "            X[item_count] = np.reshape(x, [max_len, embed_dim])\n",
    "            Y[item_count] = self.Y[i]\n",
    "            item_count += 1\n",
    "        return X[:], Y[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e64b7dc-8bd8-4536-8114-c24bb1ee68e2",
   "metadata": {},
   "source": [
    "### Anomaly Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341cace1-89e7-442e-a449-56c9fe2d7626",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim), ]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "    \n",
    "    \n",
    "def transformer_classifer(embed_dim, ff_dim, max_len, num_heads, dropout=0.1):\n",
    "    inputs = layers.Input(shape=(max_len, embed_dim))\n",
    "    transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "    embedding_layer = PositionEmbedding(100, 2000, embed_dim)\n",
    "    x = embedding_layer(inputs)\n",
    "    x = transformer_block(x)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Dense(32, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34ceeff-aa21-4ae8-bda9-0783b0247d7d",
   "metadata": {},
   "source": [
    "## Helper functions for training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8227f8-ef49-48e5-9e10-0dc6f78bc105",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator(training_generator, validate_generator, num_train_samples, num_val_samples, batch_size,\n",
    "                    epoch_num, model_name=None):\n",
    "    epochs = epoch_num\n",
    "    steps_per_epoch = num_train_samples\n",
    "    num_train_steps = steps_per_epoch * epochs\n",
    "    num_warmup_steps = int(0.1 * num_train_steps)\n",
    "\n",
    "    init_lr = 3e-4\n",
    "    optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
    "                                              num_train_steps=num_train_steps,\n",
    "                                              num_warmup_steps=num_warmup_steps,\n",
    "                                              optimizer_type='adamw')\n",
    "\n",
    "    loss_object = SparseCategoricalCrossentropy()\n",
    "\n",
    "    model = transformer_classifer(768, ff_dim=2048, max_len=75, num_heads=12, dropout=0.1)\n",
    "\n",
    "    # model.load_weights(\"hdfs_transformer.hdf5\")\n",
    "\n",
    "    model.compile(loss=loss_object, metrics=['accuracy'],\n",
    "                  optimizer=optimizer)\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "    # checkpoint\n",
    "    filepath = model_name\n",
    "    checkpoint = ModelCheckpoint(filepath,\n",
    "                                 monitor='val_accuracy',\n",
    "                                 verbose=1,\n",
    "                                 save_best_only=True,\n",
    "                                 mode='max',\n",
    "                                 save_weights_only=True)\n",
    "    early_stop = EarlyStopping(\n",
    "        monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto',\n",
    "        baseline=None, restore_best_weights=True\n",
    "    )\n",
    "    callbacks_list = [checkpoint, early_stop]\n",
    "\n",
    "    history = model.fit(training_generator,\n",
    "                        steps_per_epoch=int(num_train_samples / batch_size),\n",
    "                        epochs=epoch_num,\n",
    "                        verbose=1,\n",
    "                        validation_data=validate_generator,\n",
    "                        validation_steps=int(num_val_samples / batch_size),\n",
    "                        workers=16,\n",
    "                        max_queue_size=32,\n",
    "                        callbacks=callbacks_list,\n",
    "                        shuffle=True\n",
    "                        )\n",
    "    return model, history\n",
    "\n",
    "\n",
    "def train(X, Y, epoch_num, batch_size, model_file=None):\n",
    "    X, Y = shuffle(X, Y)\n",
    "    n_samples = len(X)\n",
    "    train_x, train_y = X[:int(n_samples * 90 / 100)], Y[:int(n_samples * 90 / 100)]\n",
    "    val_x, val_y = X[int(n_samples * 90 / 100):], Y[int(n_samples * 90 / 100):]\n",
    "\n",
    "    training_generator, num_train_samples = BatchGenerator(train_x, train_y, batch_size), len(train_x)\n",
    "    validate_generator, num_val_samples = BatchGenerator(val_x, val_y, batch_size), len(val_x)\n",
    "\n",
    "    print(\"Number of training samples: {0} - Number of validating samples: {1}\".format(num_train_samples,\n",
    "                                                                                       num_val_samples))\n",
    "\n",
    "    model, history = train_generator(training_generator, validate_generator, num_train_samples, num_val_samples, batch_size,\n",
    "                            epoch_num, model_name=model_file)\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd78708-38ca-47dc-bf9d-fa60fcd45a25",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45a9034-323f-4944-8cd1-93d9b3505adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, history = train(x_tr, y_tr, 1, 64, \"bert_hdfs_weights.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f667315d-97c7-4eda-9183-f343382c7cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {'metrics': []}\n",
    "for key, value in history.history.items():\n",
    "    metrics['metrics'].append({\n",
    "        'name': key,\n",
    "        'numberValue': value[0]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b503daca-3a90-4003-ae1b-b54d9353d7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('mlpipeline-metrics.json', 'w') as f:\n",
    "    json.dump(metrics, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645c4d81-9b60-4491-8fe5-f0ee7977c267",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
